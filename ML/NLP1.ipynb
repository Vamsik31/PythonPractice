{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in c:\\users\\vkaru\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\vkaru\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\vkaru\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\vkaru\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 20.1 MB/s eta 0:00:00\n",
      "Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl (273 kB)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.9.1 regex-2024.11.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "Python was not found; run without arguments to install from the Microsoft Store, or disable this shortcut from Settings > Apps > Advanced app settings > App execution aliases.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacyNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading spacy-3.8.4-cp312-cp312-win_amd64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.12-cp312-cp312-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp312-cp312-win_amd64.whl.metadata (8.8 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp312-cp312-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.4-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.15.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\vkaru\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\vkaru\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.2.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\vkaru\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\vkaru\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.10.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vkaru\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.1.5)\n",
      "Collecting setuptools (from spacy)\n",
      "  Using cached setuptools-75.8.2-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vkaru\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (24.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\vkaru\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\vkaru\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\vkaru\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vkaru\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vkaru\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vkaru\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vkaru\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.2.0-cp312-cp312-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\vkaru\\appdata\\roaming\\python\\python312\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\vkaru\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\vkaru\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.20.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vkaru\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\vkaru\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\vkaru\\appdata\\roaming\\python\\python312\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\vkaru\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\vkaru\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Downloading spacy-3.8.4-cp312-cp312-win_amd64.whl (11.8 MB)\n",
      "   ---------------------------------------- 0.0/11.8 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 5.5/11.8 MB 33.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.5/11.8 MB 38.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.8/11.8 MB 29.5 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp312-cp312-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.12-cp312-cp312-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp312-cp312-win_amd64.whl (122 kB)\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp312-cp312-win_amd64.whl (632 kB)\n",
      "   ---------------------------------------- 0.0/632.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 632.6/632.6 kB 23.2 MB/s eta 0:00:00\n",
      "Downloading thinc-8.3.4-cp312-cp312-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 25.4 MB/s eta 0:00:00\n",
      "Downloading typer-0.15.2-py3-none-any.whl (45 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Using cached setuptools-75.8.2-py3-none-any.whl (1.2 MB)\n",
      "Downloading blis-1.2.0-cp312-cp312-win_amd64.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   -------------------------------------- - 6.0/6.3 MB 37.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 20.2 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.20.0-py3-none-any.whl (52 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 5.4/5.4 MB 29.8 MB/s eta 0:00:00\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Downloading marisa_trie-1.2.1-cp312-cp312-win_amd64.whl (150 kB)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, smart-open, shellingham, setuptools, murmurhash, cloudpathlib, catalogue, blis, srsly, preshed, marisa-trie, typer, language-data, confection, weasel, thinc, langcodes, spacy\n",
      "Successfully installed blis-1.2.0 catalogue-2.0.10 cloudpathlib-0.20.0 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 murmurhash-1.0.12 preshed-3.0.9 setuptools-75.8.2 shellingham-1.5.4 smart-open-7.1.0 spacy-3.8.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 typer-0.15.2 wasabi-1.1.3 weasel-0.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "Python was not found; run without arguments to install from the Microsoft Store, or disable this shortcut from Settings > Apps > Advanced app settings > App execution aliases.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp312-cp312-win_amd64.whl.metadata (8.2 kB)\n",
      "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
      "  Downloading numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.13.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\vkaru\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\vkaru\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
      "Downloading gensim-4.3.3-cp312-cp312-win_amd64.whl (24.0 MB)\n",
      "   ---------------------------------------- 0.0/24.0 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 5.0/24.0 MB 27.4 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 12.1/24.0 MB 30.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 18.9/24.0 MB 31.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  23.9/24.0 MB 29.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.0/24.0 MB 24.5 MB/s eta 0:00:00\n",
      "Downloading numpy-1.26.4-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "   ---------------------------------------- 0.0/15.5 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 5.8/15.5 MB 27.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 13.1/15.5 MB 30.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.5/15.5 MB 31.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.5/15.5 MB 21.7 MB/s eta 0:00:00\n",
      "Downloading scipy-1.13.1-cp312-cp312-win_amd64.whl (45.9 MB)\n",
      "   ---------------------------------------- 0.0/45.9 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 7.1/45.9 MB 33.6 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 12.8/45.9 MB 32.2 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 18.9/45.9 MB 31.3 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 25.2/45.9 MB 30.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 30.9/45.9 MB 30.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 38.8/45.9 MB 31.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  45.9/45.9 MB 31.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  45.9/45.9 MB 31.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 45.9/45.9 MB 25.6 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy, scipy, gensim\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.3\n",
      "    Uninstalling numpy-2.2.3:\n",
      "      Successfully uninstalled numpy-2.2.3\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.15.2\n",
      "    Uninstalling scipy-1.15.2:\n",
      "      Successfully uninstalled scipy-1.15.2\n",
      "Successfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\vkaru\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\vkaru\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\vkaru\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\~cipy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\vkaru\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\~cipy'.\n",
      "  You can safely remove it manually.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: nltk>=3.9 in c:\\users\\vkaru\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\vkaru\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>=3.9->textblob) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\vkaru\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>=3.9->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\vkaru\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\vkaru\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>=3.9->textblob) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\vkaru\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk>=3.9->textblob) (0.4.6)\n",
      "Downloading textblob-0.19.0-py3-none-any.whl (624 kB)\n",
      "   ---------------------------------------- 0.0/624.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 624.3/624.3 kB 7.7 MB/s eta 0:00:00\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.19.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk\n",
    "!python3 -m nltk.downloader all\n",
    "%pip install spacy\n",
    "!python3 -m spacy download en_core_web_sm\n",
    "%pip  install gensim\n",
    "%pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vkaru\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\vkaru\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "#nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokenization: ['Hello!', 'My Name is VAMSI KRISHNA KARUMANCHI.', 'Currently I had Graduated from WICHITA STATE UNIVERSITY with a Masters Degree']\n",
      "Word Tokenization: ['Hello', '!', 'My', 'Name', 'is', 'VAMSI', 'KRISHNA', 'KARUMANCHI', '.', 'Currently', 'I', 'had', 'Graduated', 'from', 'WICHITA', 'STATE', 'UNIVERSITY', 'with', 'a', 'Masters', 'Degree']\n",
      "Sentence Tokenization: ['Vamsi is a highly skilled SQL Developer and Data Engineer with extensive experience in database design, optimization, and troubleshooting.', 'He has a strong background in SQL investigations, efficiently handling query performance tuning, indexing strategies, and stored procedures to ensure seamless data retrieval.', 'Vamsi has been instrumental in building databases from the ground up, defining schemas, and ensuring data integrity for enterprise-level applications.', 'His expertise extends to Python development, where he has worked on data pipelines, ETL workflows, and API integrations to facilitate efficient data movement across systems.', 'Vamsi has collaborated with cross-functional teams, including business analysts, product managers, and software engineers, to align technical solutions with business goals.', 'His problem-solving skills and deep understanding of relational databases allow him to troubleshoot complex production issues effectively.', 'Additionally, Vamsi has hands-on experience with PostgreSQL, MySQL, MongoDB, and cloud-based databases, making him adaptable to various data environments.', 'His proficiency in Apache Airflow and Pandas allows him to automate workflows and analyze large datasets efficiently.', 'As a dedicated professional, Vamsi is always eager to explore new technologies, optimize database performance, and contribute to the success of data-driven projects.', 'His ability to write efficient queries, streamline data processes, and ensure high availability makes him a valuable asset to any organization.']\n",
      "Word Tokenization: ['Vamsi', 'is', 'a', 'highly', 'skilled', 'SQL', 'Developer', 'and', 'Data', 'Engineer', 'with', 'extensive', 'experience', 'in', 'database', 'design', ',', 'optimization', ',', 'and', 'troubleshooting', '.', 'He', 'has', 'a', 'strong', 'background', 'in', 'SQL', 'investigations', ',', 'efficiently', 'handling', 'query', 'performance', 'tuning', ',', 'indexing', 'strategies', ',', 'and', 'stored', 'procedures', 'to', 'ensure', 'seamless', 'data', 'retrieval', '.', 'Vamsi', 'has', 'been', 'instrumental', 'in', 'building', 'databases', 'from', 'the', 'ground', 'up', ',', 'defining', 'schemas', ',', 'and', 'ensuring', 'data', 'integrity', 'for', 'enterprise-level', 'applications', '.', 'His', 'expertise', 'extends', 'to', 'Python', 'development', ',', 'where', 'he', 'has', 'worked', 'on', 'data', 'pipelines', ',', 'ETL', 'workflows', ',', 'and', 'API', 'integrations', 'to', 'facilitate', 'efficient', 'data', 'movement', 'across', 'systems', '.', 'Vamsi', 'has', 'collaborated', 'with', 'cross-functional', 'teams', ',', 'including', 'business', 'analysts', ',', 'product', 'managers', ',', 'and', 'software', 'engineers', ',', 'to', 'align', 'technical', 'solutions', 'with', 'business', 'goals', '.', 'His', 'problem-solving', 'skills', 'and', 'deep', 'understanding', 'of', 'relational', 'databases', 'allow', 'him', 'to', 'troubleshoot', 'complex', 'production', 'issues', 'effectively', '.', 'Additionally', ',', 'Vamsi', 'has', 'hands-on', 'experience', 'with', 'PostgreSQL', ',', 'MySQL', ',', 'MongoDB', ',', 'and', 'cloud-based', 'databases', ',', 'making', 'him', 'adaptable', 'to', 'various', 'data', 'environments', '.', 'His', 'proficiency', 'in', 'Apache', 'Airflow', 'and', 'Pandas', 'allows', 'him', 'to', 'automate', 'workflows', 'and', 'analyze', 'large', 'datasets', 'efficiently', '.', 'As', 'a', 'dedicated', 'professional', ',', 'Vamsi', 'is', 'always', 'eager', 'to', 'explore', 'new', 'technologies', ',', 'optimize', 'database', 'performance', ',', 'and', 'contribute', 'to', 'the', 'success', 'of', 'data-driven', 'projects', '.', 'His', 'ability', 'to', 'write', 'efficient', 'queries', ',', 'streamline', 'data', 'processes', ',', 'and', 'ensure', 'high', 'availability', 'makes', 'him', 'a', 'valuable', 'asset', 'to', 'any', 'organization', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello! My Name is VAMSI KRISHNA KARUMANCHI. Currently I had Graduated from WICHITA STATE UNIVERSITY with a Masters Degree\"\n",
    "new_text = \"\"\"Vamsi is a highly skilled SQL Developer and Data Engineer with extensive experience in database design, optimization, and troubleshooting. He has a strong background in SQL investigations, efficiently handling query performance tuning, indexing strategies, and stored procedures to ensure seamless data retrieval. Vamsi has been instrumental in building databases from the ground up, defining schemas, and ensuring data integrity for enterprise-level applications. His expertise extends to Python development, where he has worked on data pipelines, ETL workflows, and API integrations to facilitate efficient data movement across systems. Vamsi has collaborated with cross-functional teams, including business analysts, product managers, and software engineers, to align technical solutions with business goals. His problem-solving skills and deep understanding of relational databases allow him to troubleshoot complex production issues effectively. Additionally, Vamsi has hands-on experience with PostgreSQL, MySQL, MongoDB, and cloud-based databases, making him adaptable to various data environments. His proficiency in Apache Airflow and Pandas allows him to automate workflows and analyze large datasets efficiently. As a dedicated professional, Vamsi is always eager to explore new technologies, optimize database performance, and contribute to the success of data-driven projects. His ability to write efficient queries, streamline data processes, and ensure high availability makes him a valuable asset to any organization.\"\"\"\n",
    "\n",
    "print(\"Sentence Tokenization:\", sent_tokenize(text, language=\"english\"))\n",
    "print(\"Word Tokenization:\", word_tokenize(text))\n",
    "print(\"Sentence Tokenization:\", sent_tokenize(new_text, language=\"english\"))\n",
    "print(\"Word Tokenization:\", word_tokenize(new_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vamsi', 'highly', 'skilled', 'sql', 'developer', 'data', 'engineer', 'extensive', 'experience', 'database', 'design', 'optimization', 'troubleshooting', 'strong', 'background', 'sql', 'investigations', 'efficiently', 'handling', 'query', 'performance', 'tuning', 'indexing', 'strategies', 'stored', 'procedures', 'ensure', 'seamless', 'data', 'retrieval', 'vamsi', 'instrumental', 'building', 'databases', 'ground', 'defining', 'schemas', 'ensuring', 'data', 'integrity', 'enterpriselevel', 'applications', 'expertise', 'extends', 'python', 'development', 'worked', 'data', 'pipelines', 'etl', 'workflows', 'api', 'integrations', 'facilitate', 'efficient', 'data', 'movement', 'across', 'systems', 'vamsi', 'collaborated', 'crossfunctional', 'teams', 'including', 'business', 'analysts', 'product', 'managers', 'software', 'engineers', 'align', 'technical', 'solutions', 'business', 'goals', 'problemsolving', 'skills', 'deep', 'understanding', 'relational', 'databases', 'allow', 'troubleshoot', 'complex', 'production', 'issues', 'effectively', 'additionally', 'vamsi', 'handson', 'experience', 'postgresql', 'mysql', 'mongodb', 'cloudbased', 'databases', 'making', 'adaptable', 'various', 'data', 'environments', 'proficiency', 'apache', 'airflow', 'pandas', 'allows', 'automate', 'workflows', 'analyze', 'large', 'datasets', 'efficiently', 'dedicated', 'professional', 'vamsi', 'always', 'eager', 'explore', 'new', 'technologies', 'optimize', 'database', 'performance', 'contribute', 'success', 'datadriven', 'projects', 'ability', 'write', 'efficient', 'queries', 'streamline', 'data', 'processes', 'ensure', 'high', 'availability', 'makes', 'valuable', 'asset', 'organization']\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Example usage\n",
    "#sample_text = \"Natural Language Processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence.\"\n",
    "processed_tokens = preprocess_text(new_text)\n",
    "print(processed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Vamsi', 'is', 'a', 'highly', 'skilled', 'SQL', 'Developer', 'and', 'Data', 'Engineer', 'with', 'extensive', 'experience', 'in', 'database', 'design', ',', 'optimization', ',', 'and', 'troubleshooting', '.', 'He', 'has', 'a', 'strong', 'background', 'in', 'SQL', 'investigations', ',', 'efficiently', 'handling', 'query', 'performance', 'tuning', ',', 'indexing', 'strategies', ',', 'and', 'stored', 'procedures', 'to', 'ensure', 'seamless', 'data', 'retrieval', '.', 'Vamsi', 'has', 'been', 'instrumental', 'in', 'building', 'databases', 'from', 'the', 'ground', 'up', ',', 'defining', 'schemas', ',', 'and', 'ensuring', 'data', 'integrity', 'for', 'enterprise-level', 'applications', '.', 'His', 'expertise', 'extends', 'to', 'Python', 'development', ',', 'where', 'he', 'has', 'worked', 'on', 'data', 'pipelines', ',', 'ETL', 'workflows', ',', 'and', 'API', 'integrations', 'to', 'facilitate', 'efficient', 'data', 'movement', 'across', 'systems', '.', 'Vamsi', 'has', 'collaborated', 'with', 'cross-functional', 'teams', ',', 'including', 'business', 'analysts', ',', 'product', 'managers', ',', 'and', 'software', 'engineers', ',', 'to', 'align', 'technical', 'solutions', 'with', 'business', 'goals', '.', 'His', 'problem-solving', 'skills', 'and', 'deep', 'understanding', 'of', 'relational', 'databases', 'allow', 'him', 'to', 'troubleshoot', 'complex', 'production', 'issues', 'effectively', '.', 'Additionally', ',', 'Vamsi', 'has', 'hands-on', 'experience', 'with', 'PostgreSQL', ',', 'MySQL', ',', 'MongoDB', ',', 'and', 'cloud-based', 'databases', ',', 'making', 'him', 'adaptable', 'to', 'various', 'data', 'environments', '.', 'His', 'proficiency', 'in', 'Apache', 'Airflow', 'and', 'Pandas', 'allows', 'him', 'to', 'automate', 'workflows', 'and', 'analyze', 'large', 'datasets', 'efficiently', '.', 'As', 'a', 'dedicated', 'professional', ',', 'Vamsi', 'is', 'always', 'eager', 'to', 'explore', 'new', 'technologies', ',', 'optimize', 'database', 'performance', ',', 'and', 'contribute', 'to', 'the', 'success', 'of', 'data-driven', 'projects', '.', 'His', 'ability', 'to', 'write', 'efficient', 'queries', ',', 'streamline', 'data', 'processes', ',', 'and', 'ensure', 'high', 'availability', 'makes', 'him', 'a', 'valuable', 'asset', 'to', 'any', 'organization', '.']\n",
      "Filtered Words: ['Vamsi', 'highly', 'skilled', 'SQL', 'Developer', 'Data', 'Engineer', 'extensive', 'experience', 'database', 'design', ',', 'optimization', ',', 'troubleshooting', '.', 'strong', 'background', 'SQL', 'investigations', ',', 'efficiently', 'handling', 'query', 'performance', 'tuning', ',', 'indexing', 'strategies', ',', 'stored', 'procedures', 'ensure', 'seamless', 'data', 'retrieval', '.', 'Vamsi', 'instrumental', 'building', 'databases', 'ground', ',', 'defining', 'schemas', ',', 'ensuring', 'data', 'integrity', 'enterprise-level', 'applications', '.', 'expertise', 'extends', 'Python', 'development', ',', 'worked', 'data', 'pipelines', ',', 'ETL', 'workflows', ',', 'API', 'integrations', 'facilitate', 'efficient', 'data', 'movement', 'across', 'systems', '.', 'Vamsi', 'collaborated', 'cross-functional', 'teams', ',', 'including', 'business', 'analysts', ',', 'product', 'managers', ',', 'software', 'engineers', ',', 'align', 'technical', 'solutions', 'business', 'goals', '.', 'problem-solving', 'skills', 'deep', 'understanding', 'relational', 'databases', 'allow', 'troubleshoot', 'complex', 'production', 'issues', 'effectively', '.', 'Additionally', ',', 'Vamsi', 'hands-on', 'experience', 'PostgreSQL', ',', 'MySQL', ',', 'MongoDB', ',', 'cloud-based', 'databases', ',', 'making', 'adaptable', 'various', 'data', 'environments', '.', 'proficiency', 'Apache', 'Airflow', 'Pandas', 'allows', 'automate', 'workflows', 'analyze', 'large', 'datasets', 'efficiently', '.', 'dedicated', 'professional', ',', 'Vamsi', 'always', 'eager', 'explore', 'new', 'technologies', ',', 'optimize', 'database', 'performance', ',', 'contribute', 'success', 'data-driven', 'projects', '.', 'ability', 'write', 'efficient', 'queries', ',', 'streamline', 'data', 'processes', ',', 'ensure', 'high', 'availability', 'makes', 'valuable', 'asset', 'organization', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vkaru\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "text = \"Natural Language Processing is amazing. It allows machines to understand text.\"\n",
    "words = word_tokenize(new_text)\n",
    "print(words)\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "print(\"Filtered Words:\", filtered_words)\n",
    "#processed_tokens = preprocess_text(filtered_words)\n",
    "#print(\"Processed Words:\", processed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vamsi', 'highly', 'skilled', 'sql', 'developer', 'data', 'engineer', 'extensive', 'experience', 'database', 'design', 'optimization', 'troubleshooting', 'strong', 'background', 'sql', 'investigations', 'efficiently', 'handling', 'query', 'performance', 'tuning', 'indexing', 'strategies', 'stored', 'procedures', 'ensure', 'seamless', 'data', 'retrieval', 'vamsi', 'instrumental', 'building', 'databases', 'ground', 'defining', 'schemas', 'ensuring', 'data', 'integrity', 'enterpriselevel', 'applications', 'expertise', 'extends', 'python', 'development', 'worked', 'data', 'pipelines', 'etl', 'workflows', 'api', 'integrations', 'facilitate', 'efficient', 'data', 'movement', 'across', 'systems', 'vamsi', 'collaborated', 'crossfunctional', 'teams', 'including', 'business', 'analysts', 'product', 'managers', 'software', 'engineers', 'align', 'technical', 'solutions', 'business', 'goals', 'problemsolving', 'skills', 'deep', 'understanding', 'relational', 'databases', 'allow', 'troubleshoot', 'complex', 'production', 'issues', 'effectively', 'additionally', 'vamsi', 'handson', 'experience', 'postgresql', 'mysql', 'mongodb', 'cloudbased', 'databases', 'making', 'adaptable', 'various', 'data', 'environments', 'proficiency', 'apache', 'airflow', 'pandas', 'allows', 'automate', 'workflows', 'analyze', 'large', 'datasets', 'efficiently', 'dedicated', 'professional', 'vamsi', 'always', 'eager', 'explore', 'new', 'technologies', 'optimize', 'database', 'performance', 'contribute', 'success', 'datadriven', 'projects', 'ability', 'write', 'efficient', 'queries', 'streamline', 'data', 'processes', 'ensure', 'high', 'availability', 'makes', 'valuable', 'asset', 'organization']\n",
      "['vamsi', 'highli', 'skill', 'sql', 'develop', 'data', 'engin', 'extens', 'experi', 'databas', 'design', 'optim', 'troubleshoot', 'strong', 'background', 'sql', 'investig', 'effici', 'handl', 'queri', 'perform', 'tune', 'index', 'strategi', 'store', 'procedur', 'ensur', 'seamless', 'data', 'retriev', 'vamsi', 'instrument', 'build', 'databas', 'ground', 'defin', 'schema', 'ensur', 'data', 'integr', 'enterpriselevel', 'applic', 'expertis', 'extend', 'python', 'develop', 'work', 'data', 'pipelin', 'etl', 'workflow', 'api', 'integr', 'facilit', 'effici', 'data', 'movement', 'across', 'system', 'vamsi', 'collabor', 'crossfunct', 'team', 'includ', 'busi', 'analyst', 'product', 'manag', 'softwar', 'engin', 'align', 'technic', 'solut', 'busi', 'goal', 'problemsolv', 'skill', 'deep', 'understand', 'relat', 'databas', 'allow', 'troubleshoot', 'complex', 'product', 'issu', 'effect', 'addit', 'vamsi', 'handson', 'experi', 'postgresql', 'mysql', 'mongodb', 'cloudbas', 'databas', 'make', 'adapt', 'variou', 'data', 'environ', 'profici', 'apach', 'airflow', 'panda', 'allow', 'autom', 'workflow', 'analyz', 'larg', 'dataset', 'effici', 'dedic', 'profession', 'vamsi', 'alway', 'eager', 'explor', 'new', 'technolog', 'optim', 'databas', 'perform', 'contribut', 'success', 'datadriven', 'project', 'abil', 'write', 'effici', 'queri', 'streamlin', 'data', 'process', 'ensur', 'high', 'avail', 'make', 'valuabl', 'asset', 'organ']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "print(processed_tokens)\n",
    "words = processed_tokens#[\"running\", \"flies\", \"easily\", \"fairly\"]\n",
    "print([ps.stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vamsi', 'highli', 'skill', 'sql', 'develop', 'data', 'engin', 'extens', 'experi', 'databas', 'design', 'optim', 'troubleshoot', 'strong', 'background', 'sql', 'investig', 'effici', 'handl', 'queri', 'perform', 'tune', 'index', 'strategi', 'store', 'procedur', 'ensur', 'seamless', 'data', 'retriev', 'vamsi', 'instrument', 'build', 'databas', 'ground', 'defin', 'schema', 'ensur', 'data', 'integr', 'enterpriselevel', 'applic', 'expertis', 'extend', 'python', 'develop', 'work', 'data', 'pipelin', 'etl', 'workflow', 'api', 'integr', 'facilit', 'effici', 'data', 'movement', 'across', 'system', 'vamsi', 'collabor', 'crossfunct', 'team', 'includ', 'busi', 'analyst', 'product', 'manag', 'softwar', 'engin', 'align', 'technic', 'solut', 'busi', 'goal', 'problemsolv', 'skill', 'deep', 'understand', 'relat', 'databas', 'allow', 'troubleshoot', 'complex', 'product', 'issu', 'effect', 'addit', 'vamsi', 'handson', 'experi', 'postgresql', 'mysql', 'mongodb', 'cloudbas', 'databas', 'make', 'adapt', 'variou', 'data', 'environ', 'profici', 'apach', 'airflow', 'panda', 'allow', 'autom', 'workflow', 'analyz', 'larg', 'dataset', 'effici', 'dedic', 'profession', 'vamsi', 'alway', 'eager', 'explor', 'new', 'technolog', 'optim', 'databas', 'perform', 'contribut', 'success', 'datadriven', 'project', 'abil', 'write', 'effici', 'queri', 'streamlin', 'data', 'process', 'ensur', 'high', 'avail', 'make', 'valuabl', 'asset', 'organ']\n",
      "['Vamsi', 'be', 'a', 'highly', 'skilled', 'SQL', 'Developer', 'and', 'Data', 'Engineer', 'with', 'extensive', 'experience', 'in', 'database', 'design', ',', 'optimization', ',', 'and', 'troubleshooting', '.', '\\n', 'he', 'have', 'a', 'strong', 'background', 'in', 'SQL', 'investigation', ',', 'efficiently', 'handle', 'query', 'performance', 'tuning', ',', 'indexing', 'strategy', ',', 'and', 'store', 'procedure', 'to', 'ensure', 'seamless', 'datum', 'retrieval', '.', 'Vamsi', 'have', 'be', 'instrumental', 'in', 'build', 'database', 'from', 'the', 'ground', 'up', ',', 'define', 'schemas', ',', 'and', 'ensure', 'datum', 'integrity', 'for', 'enterprise', '-', 'level', 'application', '.', 'his', 'expertise', 'extend', 'to', 'Python', 'development', ',', 'where', 'he', 'have', 'work', 'on', 'datum', 'pipeline', ',', 'etl', 'workflow', ',', 'and', 'api', 'integration', 'to', 'facilitate', 'efficient', 'datum', 'movement', 'across', 'system', '.', 'Vamsi', 'have', 'collaborate', 'with', 'cross', '-', 'functional', 'team', ',', 'include', 'business', 'analyst', ',', 'product', 'manager', ',', 'and', 'software', 'engineer', ',', 'to', 'align', 'technical', 'solution', 'with', 'business', 'goal', '.', 'his', 'problem', '-', 'solve', 'skill', 'and', 'deep', 'understanding', 'of', 'relational', 'database', 'allow', 'he', 'to', 'troubleshoot', 'complex', 'production', 'issue', 'effectively', '.', 'additionally', ',', 'Vamsi', 'have', 'hand', '-', 'on', 'experience', 'with', 'PostgreSQL', ',', 'MySQL', ',', 'mongodb', ',', 'and', 'cloud', '-', 'base', 'database', ',', 'make', 'he', 'adaptable', 'to', 'various', 'datum', 'environment', '.', 'his', 'proficiency', 'in', 'Apache', 'Airflow', 'and', 'Pandas', 'allow', 'he', 'to', 'automate', 'workflow', 'and', 'analyze', 'large', 'dataset', 'efficiently', '.', 'as', 'a', 'dedicated', 'professional', ',', 'Vamsi', 'be', 'always', 'eager', 'to', 'explore', 'new', 'technology', ',', 'optimize', 'database', 'performance', ',', 'and', 'contribute', 'to', 'the', 'success', 'of', 'data', '-', 'drive', 'project', '.', 'his', 'ability', 'to', 'write', 'efficient', 'query', ',', 'streamline', 'data', 'process', ',', 'and', 'ensure', 'high', 'availability', 'make', 'he', 'a', 'valuable', 'asset', 'to', 'any', 'organization', '.', '\\n']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "input_string = \"\"\"Vamsi is a highly skilled SQL Developer and Data Engineer with extensive experience in database design, optimization, and troubleshooting. \n",
    "He has a strong background in SQL investigations, efficiently handling query performance tuning, indexing strategies, and stored procedures to ensure seamless data retrieval. Vamsi has been instrumental in building databases from the ground up, defining schemas, and ensuring data integrity for enterprise-level applications. His expertise extends to Python development, where he has worked on data pipelines, ETL workflows, and API integrations to facilitate efficient data movement across systems. Vamsi has collaborated with cross-functional teams, including business analysts, product managers, and software engineers, to align technical solutions with business goals. His problem-solving skills and deep understanding of relational databases allow him to troubleshoot complex production issues effectively. Additionally, Vamsi has hands-on experience with PostgreSQL, MySQL, MongoDB, and cloud-based databases, making him adaptable to various data environments. His proficiency in Apache Airflow and Pandas allows him to automate workflows and analyze large datasets efficiently. As a dedicated professional, Vamsi is always eager to explore new technologies, optimize database performance, and contribute to the success of data-driven projects. His ability to write efficient queries, streamline data processes, and ensure high availability makes him a valuable asset to any organization.\n",
    "\"\"\"\n",
    "doc = nlp(input_string) #\"running flies easily fairly\")\n",
    "print([ps.stem(word) for word in words])\n",
    "print([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vkaru\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "better : good\n"
     ]
    }
   ],
   "source": [
    "# import these modules\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    " \n",
    "lemmatizer = WordNetLemmatizer()\n",
    " \n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
    " \n",
    "# a denotes adjective in \"pos\"\n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos=\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'fly', 'easily', 'fairly']\n",
      "running\n",
      "fly\n",
      "easily\n",
      "fairly\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"running flies easily fairly\")\n",
    "print([token.lemma_ for token in doc])\n",
    "\n",
    "print(lemmatizer.lemmatize(\"running\"))\n",
    "print(lemmatizer.lemmatize(\"flies\"))\n",
    "print(lemmatizer.lemmatize(\"easily\"))\n",
    "print(lemmatizer.lemmatize(\"fairly\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['fun' 'is' 'language' 'learning' 'love' 'natural' 'nlp' 'processing']\n",
      "BoW Representation:\n",
      " [[1 1 1 0 0 1 0 1]\n",
      " [0 0 0 1 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\"Natural Language Processing is fun!\", \"I love learning NLP.\"]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "print(\"BoW Representation:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fun' 'is' 'language' 'learning' 'love' 'natural' 'nlp' 'processing']\n",
      "TF-IDF Representation:\n",
      " [[0.4472136  0.4472136  0.4472136  0.         0.         0.4472136\n",
      "  0.         0.4472136 ]\n",
      " [0.         0.         0.         0.57735027 0.57735027 0.\n",
      "  0.57735027 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF Representation:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00053623  0.00023643  0.00510335  0.00900927 -0.00930295]\n",
      "[('processing', 0.21617145836353302), ('love', 0.09291718155145645), ('natural', 0.07963486760854721)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Tokenized sentences\n",
    "sentences = [\n",
    "[\"natural\", \"language\", \"processing\", \"fascinating\"],\n",
    "[\"love\", \"working\", \"text\", \"data\"],\n",
    "[\"machine\", \"learning\", \"models\", \"understand\", \"text\"]\n",
    "]\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "# Get vector for a word\n",
    "vector = model.wv['text']\n",
    "print(vector[:5]) # Print first 5 dimensions\n",
    "\n",
    "# Find similar words\n",
    "similar_words = model.wv.most_similar('text', topn=3)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity: 0.09393939393939393\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = \"The film follows the life of Alabama native Forest Gump, a good man with an ideally low IQ of 75, as historical events occur through his eyes. Jenny Curran was one of Forrest Gump’s few childhood friends and his first and only romantic interest. Throughout the film, Gump regularly experiences adversity but never loses his positive outlook. He may have been physically impaired, but he always retained sight of the big picture. He overcame adversity with his mother’s support and received a football scholarship.\"\n",
    "sentiment = TextBlob(text).sentiment\n",
    "print(\"Polarity:\", sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barack Obama PERSON\n",
      "Hawaii GPE\n",
      "USA GPE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Barack Obama was born in Hawaii and was the President of the USA.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: ['spam']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "X = [\"Win a free iPhone now!\", \"Meeting at 5 PM\", \"Exclusive offer just for you\"]\n",
    "y = [\"spam\", \"ham\", \"spam\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "pipeline = Pipeline([('vectorizer', CountVectorizer()),('classifier', MultinomialNB())])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"Prediction:\", pipeline.predict([\"Get your free gift now!\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JD Vance PERSON\n",
      "Donald Trump PERSON\n",
      "Friday DATE\n",
      "Ukrainian NORP\n",
      "Volodymyr Zelenskyy PERSON\n",
      "Trump ORG\n",
      "Vance PERSON\n",
      "the Oval Office ORG\n",
      "Friday DATE\n",
      "Zelenskyy ORG\n",
      "Ukraine GPE\n",
      "Trump ORG\n",
      "Ukrainian NORP\n",
      "the White House ORG\n",
      "Fox News Digital ORG\n",
      "Trump PERSON\n",
      "Vance PERSON\n",
      "Zelenskyy ORG\n",
      "Friday DATE\n",
      "Zelenskyy ORG\n",
      "Vance PERSON\n",
      "Trump PERSON\n",
      "America GPE\n",
      "First ORDINAL\n"
     ]
    }
   ],
   "source": [
    "newsarticle = \"\"\"Vice President JD Vance defended President Donald Trump and his administration’s foreign policy agenda Friday during a tense exchange with Ukrainian President Volodymyr Zelenskyy — inserting himself into a spotlight rarely seen by vice presidents. \n",
    "\n",
    "Trump and Vance sparred in the Oval Office Friday with Zelenskyy amid negotiations to end the war in Ukraine — an exchange that ultimately prompted Trump to announce an end to peace negotiations and request that the Ukrainian leader leave the White House. \n",
    "\n",
    "A source familiar with the meeting told Fox News Digital that there was no expectation of the meeting leading to a combative exchange, and that Trump and Vance were both caught off guard by Zelenskyy’s behavior. \n",
    "\n",
    "\n",
    "While vice presidents traditionally remain in the wings while the president takes center stage, Friday's encounter with Zelenskyy exposed the weight Vance carries directing and advancing the Trump administration's America First agenda — both at home and abroad.\n",
    "\n",
    "\"\"\"\n",
    "#doc = nlp(\"Barack Obama was born in Hawaii and was the President of the USA.\")\n",
    "doc = nlp(newsarticle)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
